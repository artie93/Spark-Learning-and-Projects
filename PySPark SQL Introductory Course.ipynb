{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b17c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"trainsched.txt\", header=True)\n",
    "\n",
    "# Create temporary table called table1\n",
    "df.createOrReplaceTempView('table1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d4c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the columns in the table df\n",
    "spark.sql(\"DESCRIBE schedule\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e98151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add col running_total that sums diff_min col in each group\n",
    "query = \"\"\"\n",
    "SELECT train_id, station, time, diff_min,\n",
    "SUM(diff_min) OVER (PARTITION BY train_id ORDER BY time) AS running_total\n",
    "FROM schedule\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and display the result\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ce208",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SELECT \n",
    "ROW_NUMBER() OVER (ORDER BY time) AS row,\n",
    "train_id, \n",
    "station, \n",
    "time, \n",
    "LEAD(time,1) OVER (ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\"\n",
    "spark.sql(query).show()\n",
    "\n",
    "# Give the number of the bad row as an integer\n",
    "bad_row = 7\n",
    "\n",
    "# Provide the missing clause, SQL keywords in upper case\n",
    "clause = 'PARTITION BY train_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab85863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the identical result in each command\n",
    "spark.sql('SELECT train_id, MIN(time) AS start FROM schedule GROUP BY train_id').show()\n",
    "df.groupBy('train_id').agg({'time':'MIN'}).withColumnRenamed('min(time)', 'start').show()\n",
    "\n",
    "# Print the second column of the result\n",
    "spark.sql('SELECT train_id, MIN(time), MAX(time) FROM schedule GROUP BY train_id').show()\n",
    "result = df.groupBy('train_id').agg({'time':'min', 'time':'max'})\n",
    "result.show()\n",
    "print(result.columns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd20a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a SQL query giving a result identical to dot_df\n",
    "query = \"SELECT train_id, MIN(time) AS start, MAX(time) AS end FROM schedule GROUP BY train_id\"\n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5950e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the identical result using dot notation \n",
    "dot_df = df.withColumn('time_next', lead('time', 1)\n",
    "        .over(Window.partitionBy('train_id')\n",
    "        .orderBy('time')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SQL query to obtain an identical result to dot_df\n",
    "query = \"\"\"\n",
    "SELECT *, \n",
    "(UNIX_TIMESTAMP(LEAD(time, 1) OVER (PARTITION BY train_id ORDER BY time),'H:m') \n",
    " - UNIX_TIMESTAMP(time, 'H:m'))/60 AS diff_min \n",
    "FROM schedule \n",
    "\"\"\"\n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbb05d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe\n",
    "df = spark.read.load('sherlock_sentences.parquet')\n",
    "\n",
    "# Filter and show the first 5 rows\n",
    "df.where('id > 70').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db3fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the clause column into a column called words \n",
    "split_df = clauses_df.select(split('clause', ' ').alias('words'))\n",
    "split_df.show(5, truncate=False)\n",
    "\n",
    "# Explode the words column into a column called word \n",
    "exploded_df = split_df.select(explode('words').alias('word'))\n",
    "exploded_df.show(10)\n",
    "\n",
    "# Count the resulting number of rows in exploded_df\n",
    "print(\"\\nNumber of rows: \", exploded_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c15f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word for each row, previous two and subsequent two words\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "part,\n",
    "LAG(word, 2) OVER(PARTITION BY part ORDER BY id) AS w1,\n",
    "LAG(word, 1) OVER(PARTITION BY part ORDER BY id) AS w2,\n",
    "word AS w3,\n",
    "LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) AS w4,\n",
    "LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) AS w5\n",
    "FROM text\n",
    "\"\"\"\n",
    "spark.sql(query).where(\"part = 12\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5196ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition text_df into 12 partitions on 'chapter' column\n",
    "repart_df = text_df.repartition(12, 'chapter')\n",
    "\n",
    "# Prove that repart_df has 12 partitions\n",
    "repart_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 10 sequences of five words\n",
    "query = \"\"\"\n",
    "SELECT w1, w2, w3, w4, w5, COUNT(*) AS count FROM (\n",
    "   SELECT word AS w1,\n",
    "   LEAD(word,1) OVER(PARTITION BY part ORDER BY id) AS w2,\n",
    "   LEAD(word,2) OVER(PARTITION BY part ORDER BY id) AS w3,\n",
    "   LEAD(word,3) OVER(PARTITION BY part ORDER BY id) AS w4,\n",
    "   LEAD(word,4) OVER(PARTITION BY part ORDER BY id) AS w5\n",
    "   FROM text\n",
    ")\n",
    "GROUP BY w1, w2, w3, w4, w5\n",
    "ORDER BY count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "df = spark.sql(query)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d1f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique 5-tuples sorted in descending order\n",
    "query = \"\"\"\n",
    "SELECT w1, w2, w3, w4, w5 FROM (\n",
    "   SELECT word AS w1,\n",
    "   LEAD(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\n",
    "   LEAD(word,2) OVER(PARTITION BY part ORDER BY id ) AS w3,\n",
    "   LEAD(word,3) OVER(PARTITION BY part ORDER BY id ) AS w4,\n",
    "   LEAD(word,4) OVER(PARTITION BY part ORDER BY id ) AS w5\n",
    "   FROM text\n",
    ")\n",
    "ORDER BY w1 DESC, w2 DESC, w3 DESC, w4 DESC, w5 DESC \n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "df = spark.sql(query)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Most frequent 3-tuple per chapter\n",
    "query = \"\"\"\n",
    "SELECT chapter, w1, w2, w3, count FROM\n",
    "(\n",
    "  SELECT\n",
    "  chapter,\n",
    "  ____() OVER (PARTITION BY chapter ORDER BY ____ DESC) AS row,\n",
    "  w1, w2, w3, count\n",
    "  FROM ( %s )\n",
    ")\n",
    "WHERE row = ____\n",
    "ORDER BY chapter ASC\n",
    "\"\"\" % subquery\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72717e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpersists df1 and df2 and initializes a timer\n",
    "prep(df1, df2) \n",
    "\n",
    "# Cache df1\n",
    "df1.cache()\n",
    "\n",
    "# Run actions on both dataframes\n",
    "run(df1, \"df1_1st\") \n",
    "run(df1, \"df1_2nd\")\n",
    "run(df2, \"df2_1st\")\n",
    "run(df2, \"df2_2nd\", elapsed=True)\n",
    "\n",
    "# Prove df1 is cached\n",
    "print(df1.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05592c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpersist df1 and df2 and initializes a timer\n",
    "prep(df1, df2) \n",
    "\n",
    "# Persist df2 using memory and disk storage level \n",
    "df2.persist(storageLevel=pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Run actions both dataframes\n",
    "run(df1, \"df1_1st\") \n",
    "run(df1, \"df1_2nd\") \n",
    "run(df2, \"df2_1st\") \n",
    "run(df2, \"df2_2nd\", elapsed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the tables\n",
    "print(\"Tables:\\n\", spark.catalog.listTables())\n",
    "\n",
    "# Cache table1 and Confirm that it is cached\n",
    "spark.catalog.cacheTable('table1')\n",
    "print(\"table1 is cached: \", spark.catalog.isCached('table1'))\n",
    "\n",
    "# Uncache table1 and confirm that it is uncached\n",
    "spark.catalog.uncacheTable('table1')\n",
    "print(\"table1 is cached: \", spark.catalog.isCached('table1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb21a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log columns of text_df as debug message\n",
    "logging.debug(\"text_df columns: %s\", text_df.columns)\n",
    "\n",
    "# Log whether table1 is cached as info message\n",
    "logging.info(\"table1 is cached: %s\", spark.catalog.isCached(tableName=\"table1\"))\n",
    "\n",
    "# Log first row of text_df as warning message\n",
    "logging.warning(\"The first row of text_df:\\n %s\", text_df.first())\n",
    "\n",
    "# Log selected columns of text_df as error message\n",
    "logging.error(\"Selected columns: %s\", text_df.select(\"id\", \"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb22a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run explain on text_df\n",
    "text_df.explain()\n",
    "\n",
    "# Run explain on \"SELECT COUNT(*) AS count FROM table1\" \n",
    "spark.sql(\"SELECT COUNT(*) AS count FROM table1\").explain()\n",
    "\n",
    "# Run explain on \"SELECT COUNT(DISTINCT word) AS words FROM table1\"\n",
    "spark.sql(\"SELECT COUNT(DISTINCT word) AS words FROM table1\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac5ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns true if the value is a nonempty vector\n",
    "nonempty_udf = udf(lambda x:  \n",
    "    True if (x and hasattr(x, \"toArray\") and x.numNonzeros())\n",
    "    else False, BooleanType())\n",
    "\n",
    "# Returns first element of the array as string\n",
    "s_udf = udf(lambda x: str(x[0]) if (x and type(x) is list and len(x) > 0)\n",
    "    else '', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b76ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the rows where doc contains the item '5'\n",
    "df_before.where(array_contains('doc', '5')).show()\n",
    "\n",
    "# UDF removes items in TRIVIAL_TOKENS from array\n",
    "rm_trivial_udf = udf(lambda x:\n",
    "                     list(set(x) - TRIVIAL_TOKENS) if x\n",
    "                     else x,\n",
    "                     ArrayType(StringType()))\n",
    "\n",
    "# Remove trivial tokens from 'in' and 'out' columns of df2\n",
    "df_after = df_before.withColumn('in', rm_trivial_udf('in'))\\\n",
    "                    .withColumn('out', rm_trivial_udf('out'))\n",
    "\n",
    "# Show the rows of df_after where doc contains the item '5'\n",
    "df_after.where(array_contains('doc','5')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects the first element of a vector column\n",
    "first_udf = udf(lambda x:\n",
    "            float(x.indices[0]) \n",
    "            if (x and hasattr(x, \"toArray\") and x.numNonzeros())\n",
    "            else 0.0,\n",
    "            FloatType())\n",
    "\n",
    "# Apply first_udf to the output column\n",
    "df.select(first_udf(\"output\").alias(\"result\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add label by applying the get_first_udf to output column\n",
    "df_new = df.withColumn('label', get_first_udf('output'))\n",
    "\n",
    "# Show the first five rows \n",
    "df_new.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2801a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform df using model\n",
    "result = model.transform(df.withColumnRenamed('in', 'words'))\\\n",
    "        .withColumnRenamed('words', 'in')\\\n",
    "        .withColumnRenamed('vec', 'invec')\n",
    "result.drop('sentence').show(3, False)\n",
    "\n",
    "# Add a column based on the out column called outvec\n",
    "result = model.transform(result.withColumnRenamed('out', 'words'))\\\n",
    "        .withColumnRenamed('words', 'out')\\\n",
    "        .withColumnRenamed('vec', 'outvec')\n",
    "result.select('invec', 'outvec').show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a1a6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01648e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75353da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fcae8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786dd41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3710a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63f9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b594305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b613c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786efae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
